{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for SegFormer usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets accelerate evaluate pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "from transformers import Trainer, TrainingArguments, default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the built-in segmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a SegFormer-B0 model already fine-tuned on ADE20K\n",
    "segmenter = pipeline(\n",
    "    \"image-segmentation\",\n",
    "    model=\"optimum/segformer-b0-fintuned-ade-512-512\",\n",
    ") # uses ONNXRuntime under the hood for speed :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "# Segment your image (PIL Image, NumPy array, or URL)\n",
    "output = segmenter(\"path/to/your/image.jpg\")\n",
    "\n",
    "# output is a list of dicts: [{\"label\":\"water\",\"mask\":<PIL.Image>}, ...]\n",
    "for obj in output:\n",
    "    print(obj[\"label\"], obj[\"score\"])\n",
    "    obj[\"mask\"].save(f\"{obj['label']}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune SegFormer on Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset on huggingface (should prepare from online download)\n",
    "# Structure a dataset dict or use the datasets library to load images + mask PNGs, with columns {\"image\":…, \"label\":…}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess & tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    imgs = [img.convert(\"RGB\") for img in examples[\"image\"]]\n",
    "    masks = examples[\"label\"]  # (H×W) integer mask\n",
    "    inputs = feature_extractor(\n",
    "        images=imgs,\n",
    "        segmentation_maps=masks,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"pixel_values\": inputs[\"pixel_values\"],\n",
    "        \"labels\": inputs[\"labels\"],\n",
    "    }\n",
    "\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\n",
    "    \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    ")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
    "    num_labels=3,                    # your classes: water/sky/obstacle\n",
    "    ignore_mismatched_sizes=True,    # in case you change decoder head size\n",
    ")\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    preprocess, batched=True, remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Trainer and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"segformer-maritime\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
